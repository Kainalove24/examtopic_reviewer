id,type,text,question_images,answer_images,options,answers,,explanation
1,mcq,"Case Study -A company is building a web-based AI application by using Amazon SageMaker. The application will provide the following capabilities and features: ML experimentation, training, a central model registry, model deployment, and model monitoring.The application must ensure secure and isolated use of training data during the ML lifecycle. The training data is stored in Amazon S3.The company needs to use the central model registry to manage different versions of models in the application.Wh...",,,A.Create a separate Amazon Elastic Container Registry (Amazon ECR) repository for each model.|B.Use Amazon Elastic Container Registry (Amazon ECR) and unique tags for each model version.|C.Use the SageMaker Model Registry and model groups to catalog the models.|D.Use the SageMaker Model Registry and unique tags for each model version.,C,,
2,mcq,"Case Study -A company is building a web-based AI application by using Amazon SageMaker. The application will provide the following capabilities and features: ML experimentation, training, a central model registry, model deployment, and model monitoring.The application must ensure secure and isolated use of training data during the ML lifecycle. The training data is stored in Amazon S3.The company is experimenting with consecutive training jobs.How can the company MINIMIZE infrastructure startup ...",,,A.Use Managed Spot Training.|B.Use SageMaker managed warm pools.|C.Use SageMaker Training Compiler.|D.Use the SageMaker distributed data parallelism (SMDDP) library.,B,,
3,mcq,"Case Study -A company is building a web-based AI application by using Amazon SageMaker. The application will provide the following capabilities and features: ML experimentation, training, a central model registry, model deployment, and model monitoring.The application must ensure secure and isolated use of training data during the ML lifecycle. The training data is stored in Amazon S3.The company must implement a manual approval-based workflow to ensure that only approved models can be deployed ...",,,"A.Use SageMaker Experiments to facilitate the approval process during model registration.|B.Use SageMaker ML Lineage Tracking on the central model registry. Create tracking entities for the approval process.|C.Use SageMaker Model Monitor to evaluate the performance of the model and to manage the approval.|D.Use SageMaker Pipelines. When a model version is registered, use the AWS SDK to change the approval status to ""Approved.""",D,,
4,mcq,"Case Study -A company is building a web-based AI application by using Amazon SageMaker. The application will provide the following capabilities and features: ML experimentation, training, a central model registry, model deployment, and model monitoring.The application must ensure secure and isolated use of training data during the ML lifecycle. The training data is stored in Amazon S3.The company needs to run an on-demand workflow to monitor bias drift for models that are deployed to real-time e...",,,A.Configure the application to invoke an AWS Lambda function that runs a SageMaker Clarify job.|B.Invoke an AWS Lambda function to pull the sagemaker-model-monitor-analyzer built-in SageMaker image.|C.Use AWS Glue Data Quality to monitor bias.|D.Use SageMaker notebooks to compare the bias.,A,,
5,hotspot,HOTSPOT -A company stores historical data in .csv files in Amazon S3. Only some of the rows and columns in the .csv files are populated. The columns are not labeled. An ML engineer needs to prepare and store the data so that the company can use the data to train ML models.Select and order the correct steps from the following list to perform this task. Each step should be selected one time or not at all. (Select and order three.),https://img.examtopics.com/aws-certified-machine-learning-engineer-associate-mla-c01/image1.png,https://img.examtopics.com/aws-certified-machine-learning-engineer-associate-mla-c01/image2.png,Create an Amazon SageMaker batch tranform job for data cleaning and feature engineering.|Store the resulting data back in the Amazon S3.|Use AWS Glue crawlers to infer the schemas and available columns.|Use AWS Glue DataBrew for data cleaning and feature engineering.,Use AWS Glue crawlers to infer the schemas and available columns.|Use AWS Glue DataBrew for data cleaning and feature engineering.|Store the resulting data back in the Amazon S3.|,,
